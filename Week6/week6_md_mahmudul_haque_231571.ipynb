{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openpyxl\n",
    "# !pip install gensim\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from gensim.corpora import dictionary \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Resting is part of training. I've confirmed wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Think of talking to yourself as a tool to coac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>The clock is ticking for the United States to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>If you want to be busy, keep trying to be perf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>First, the bad news: Soda bread, corned beef a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                  short_description\n",
       "0  WELLNESS  Resting is part of training. I've confirmed wh...\n",
       "1  WELLNESS  Think of talking to yourself as a tool to coac...\n",
       "2  WELLNESS  The clock is ticking for the United States to ...\n",
       "3  WELLNESS  If you want to be busy, keep trying to be perf...\n",
       "4  WELLNESS  First, the bad news: Soda bread, corned beef a..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"NewsCategorizer.xlsx\", usecols=[\"category\", \"short_description\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['WELLNESS', 'POLITICS', 'ENTERTAINMENT', 'TRAVEL',\n",
       "       'STYLE & BEAUTY', 'PARENTING', 'FOOD & DRINK', 'WORLD NEWS',\n",
       "       'BUSINESS', 'SPORTS'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"category\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category             50000\n",
       "short_description    50000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying on basic preprocessing:\n",
    "# 1. cleaning: extracting alphabets and \n",
    "# splitting into word list for each row;\n",
    "# 2. to lowercase; 3. removing stopwords;\n",
    "# and 4. Lemmatization\n",
    "def clean_text(sentence: str) -> list:\n",
    "    \"\"\"cleaning text by extracting alphabets and\n",
    "    then splitting into word list for each sentence\n",
    "\n",
    "    Args:\n",
    "        sentence (str): sentence string\n",
    "\n",
    "    Returns:\n",
    "        list: list of words.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r\"[A-Za-z]+\")\n",
    "    return re.findall(pattern=pattern, string=sentence)\n",
    "\n",
    "def to_lowercase(word_list: list) -> list:\n",
    "    \"\"\"case changing of all contents in each list\n",
    "    of words\n",
    "\n",
    "    Args:\n",
    "        word_list (list): list of words with alphabets only texts\n",
    "\n",
    "    Returns:\n",
    "        word_list (list): list of words with lowercase transformation\n",
    "    \"\"\"\n",
    "    temp_list = []\n",
    "    for i, word in enumerate(word_list):\n",
    "        temp_list.append(word.lower())\n",
    "    word_list = temp_list[:]\n",
    "    return word_list\n",
    "\n",
    "def remove_stopwords(list_of_words: list) -> list:\n",
    "    \"\"\"removing stop words from list of words by matching\n",
    "    English stop words and extracting those out \n",
    "    \n",
    "    Args:\n",
    "        list_of_words (list): list of words with stop words\n",
    "\n",
    "    Returns:\n",
    "        list: stop word free list of words\n",
    "    \"\"\"\n",
    "    stopword_list = stopwords.words(\"english\")\n",
    "\n",
    "    return [word for word in list_of_words if word not in stopword_list]\n",
    "\n",
    "def extract_word_lemma(list_of_words: list) -> list:\n",
    "    \"\"\"lemmatization of words from list of words\n",
    "    using NLTK WordNetLemmatizer class\n",
    "\n",
    "    Args:\n",
    "        list_of_list_words (list): list of words inside a list\n",
    "\n",
    "    Returns:\n",
    "        list: lemmatized list of list of words\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in list_of_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df[\"clean_text\"] = copy_df[\"short_description\"].apply(lambda text: clean_text(text))\n",
    "copy_df[\"clean_text\"] = copy_df[\"clean_text\"].apply(lambda word_list: to_lowercase(word_list))\n",
    "copy_df[\"clean_text\"] = copy_df[\"clean_text\"].apply(lambda word_list: remove_stopwords(word_list))\n",
    "copy_df[\"clean_text\"] = copy_df[\"clean_text\"].apply(lambda word_list: remove_stopwords(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Resting is part of training. I've confirmed wh...</td>\n",
       "      <td>[resting, part, training, confirmed, sort, alr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Think of talking to yourself as a tool to coac...</td>\n",
       "      <td>[think, talking, tool, coach, challenge, narra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>The clock is ticking for the United States to ...</td>\n",
       "      <td>[clock, ticking, united, states, find, cure, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>If you want to be busy, keep trying to be perf...</td>\n",
       "      <td>[want, busy, keep, trying, perfect, want, happ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>First, the bad news: Soda bread, corned beef a...</td>\n",
       "      <td>[first, bad, news, soda, bread, corned, beef, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                  short_description  \\\n",
       "0  WELLNESS  Resting is part of training. I've confirmed wh...   \n",
       "1  WELLNESS  Think of talking to yourself as a tool to coac...   \n",
       "2  WELLNESS  The clock is ticking for the United States to ...   \n",
       "3  WELLNESS  If you want to be busy, keep trying to be perf...   \n",
       "4  WELLNESS  First, the bad news: Soda bread, corned beef a...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  [resting, part, training, confirmed, sort, alr...  \n",
       "1  [think, talking, tool, coach, challenge, narra...  \n",
       "2  [clock, ticking, united, states, find, cure, t...  \n",
       "3  [want, busy, keep, trying, perfect, want, happ...  \n",
       "4  [first, bad, news, soda, bread, corned, beef, ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = copy_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:  0\n",
      "Value:  already\n",
      "\n",
      "Key:  1\n",
      "Value:  also\n",
      "\n",
      "Key:  2\n",
      "Value:  built\n",
      "\n",
      "Key:  3\n",
      "Value:  confirmed\n",
      "\n",
      "Key:  4\n",
      "Value:  cross\n",
      "\n",
      "Key:  5\n",
      "Value:  days\n",
      "\n",
      "Key:  6\n",
      "Value:  five\n",
      "\n",
      "Key:  7\n",
      "Value:  foam\n",
      "\n",
      "Key:  8\n",
      "Value:  hard\n",
      "\n",
      "Key:  9\n",
      "Value:  knew\n",
      "\n",
      "Key:  10\n",
      "Value:  lots\n",
      "\n",
      "Key:  11\n",
      "Value:  part\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# getting dictionary of indexes with corresponding\n",
    "# word from \"clean_text\" column\n",
    "word_dict = dictionary.Dictionary(copy_df[\"clean_text\"])\n",
    "for key, value in (word_dict.iteritems()):\n",
    "    print(\"Key: \", key)\n",
    "    print(\"Value: \", value)\n",
    "    print()\n",
    "    if key>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating document to bag of words\n",
    "copy_df[\"doc2bow\"] = copy_df[\"clean_text\"].apply(lambda word_list: word_dict.doc2bow(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (3, 2),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 1),\n",
       " (13, 1),\n",
       " (14, 1),\n",
       " (15, 1),\n",
       " (16, 1),\n",
       " (17, 1),\n",
       " (18, 1),\n",
       " (19, 1),\n",
       " (20, 1),\n",
       " (21, 2),\n",
       " (22, 1),\n",
       " (23, 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copy_df[\"doc2bow\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By K=10 means same as total number of topics\n",
    "# available in our dataset\n",
    "# and iterations = 200 means the parameter\n",
    "# passes need to be set 200 to increase the\n",
    "# probability\n",
    "K = 10\n",
    "iterations = 200\n",
    "lda_model = LdaModel(\n",
    "    corpus=copy_df[\"doc2bow\"],\n",
    "    id2word=word_dict,\n",
    "    num_topics=K,\n",
    "    passes=iterations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.015*\"like\" + 0.014*\"one\" + 0.014*\"time\" + 0.013*\"get\" + 0.010*\"make\" + 0.010*\"us\" + 0.010*\"way\" + 0.009*\"know\" + 0.008*\"good\" + 0.008*\"life\"')\n",
      "(1, '0.013*\"check\" + 0.011*\"want\" + 0.011*\"style\" + 0.011*\"team\" + 0.010*\"government\" + 0.010*\"sure\" + 0.006*\"fans\" + 0.006*\"win\" + 0.006*\"economy\" + 0.006*\"games\"')\n",
      "(2, '0.009*\"women\" + 0.007*\"school\" + 0.007*\"health\" + 0.007*\"found\" + 0.007*\"study\" + 0.006*\"market\" + 0.006*\"high\" + 0.006*\"university\" + 0.006*\"new\" + 0.006*\"business\"')\n",
      "(3, '0.010*\"world\" + 0.010*\"people\" + 0.008*\"social\" + 0.008*\"one\" + 0.007*\"media\" + 0.007*\"facebook\" + 0.007*\"obama\" + 0.007*\"child\" + 0.006*\"killed\" + 0.005*\"president\"')\n",
      "(4, '0.023*\"year\" + 0.015*\"years\" + 0.014*\"first\" + 0.013*\"two\" + 0.012*\"new\" + 0.012*\"last\" + 0.011*\"one\" + 0.010*\"old\" + 0.009*\"time\" + 0.008*\"world\"')\n",
      "(5, '0.011*\"former\" + 0.008*\"said\" + 0.008*\"war\" + 0.006*\"financial\" + 0.006*\"party\" + 0.006*\"bowl\" + 0.006*\"super\" + 0.005*\"violence\" + 0.005*\"military\" + 0.005*\"deal\"')\n",
      "(6, '0.013*\"fashion\" + 0.010*\"video\" + 0.008*\"daughter\" + 0.008*\"man\" + 0.006*\"watch\" + 0.006*\"police\" + 0.005*\"service\" + 0.005*\"french\" + 0.005*\"kid\" + 0.005*\"held\"')\n",
      "(7, '0.009*\"th\" + 0.008*\"story\" + 0.007*\"one\" + 0.007*\"parents\" + 0.006*\"late\" + 0.006*\"different\" + 0.006*\"anything\" + 0.006*\"woman\" + 0.005*\"history\" + 0.005*\"words\"')\n",
      "(8, '0.013*\"state\" + 0.012*\"states\" + 0.012*\"twitter\" + 0.012*\"u\" + 0.010*\"united\" + 0.009*\"huffpost\" + 0.007*\"china\" + 0.007*\"house\" + 0.007*\"economic\" + 0.007*\"security\"')\n",
      "(9, '0.023*\"new\" + 0.016*\"city\" + 0.011*\"york\" + 0.008*\"sunday\" + 0.006*\"state\" + 0.006*\"st\" + 0.006*\"north\" + 0.006*\"black\" + 0.005*\"week\" + 0.005*\"street\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in lda_model.print_topics():\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = copy_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf(corpus: list) -> list:\n",
    "    \"\"\"calculating tf-idf of each list from given\n",
    "    list of word corpus.\n",
    "\n",
    "    from the list of words, calculate\n",
    "    tf-idf matrix and return that matrix.\n",
    "\n",
    "    Args:\n",
    "        list_of_list_words (list): list of words\n",
    "\n",
    "    Returns:\n",
    "        list: each doc based top words after tf-idf calculation\n",
    "    \"\"\"\n",
    "    # min_df = 2 is set to avoid 1 single string\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=2)\n",
    "    return tfidf_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    resting part training confirmed sort already k...\n",
       "1    think talking tool coach challenge narrate exp...\n",
       "2    clock ticking united states find cure team wor...\n",
       "3    want busy keep trying perfect want happy focus...\n",
       "4    first bad news soda bread corned beef beer hig...\n",
       "5    carey moss youbeauty com love rom coms love so...\n",
       "6         nation general scored scale little bit score\n",
       "7    also worth remembering water seaweed comes con...\n",
       "8    look culture eating behavior certainly looks l...\n",
       "9    fran ois marie arouet th century french author...\n",
       "Name: tfidf_corpus, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copy_df[\"tfidf_corpus\"] = [\" \".join(word_list) for word_list in copy_df[\"clean_text\"]]\n",
    "copy_df[\"tfidf_corpus\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tfidf_matrix = calculate_tfidf(copy_df[\"tfidf_corpus\"])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_arr = tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tfidf_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arfan/anaconda3/envs/tada/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MiniBatchKMeans(batch_size=8000, n_clusters=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MiniBatchKMeans</label><div class=\"sk-toggleable__content\"><pre>MiniBatchKMeans(batch_size=8000, n_clusters=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MiniBatchKMeans(batch_size=8000, n_clusters=10)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kmeans = KMeans(n_clusters=10, max_iter=2, n_init=1)\n",
    "kmeans = MiniBatchKMeans(n_clusters=10, max_iter=100, batch_size=8000)\n",
    "kmeans.fit(tfidf_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 is in cluster label 8\n",
      "Document 2 is in cluster label 10\n",
      "Document 3 is in cluster label 10\n",
      "Document 4 is in cluster label 10\n",
      "Document 5 is in cluster label 7\n",
      "Document 6 is in cluster label 3\n",
      "Document 7 is in cluster label 10\n",
      "Document 8 is in cluster label 10\n",
      "Document 9 is in cluster label 1\n",
      "Document 10 is in cluster label 10\n",
      "Document 11 is in cluster label 7\n",
      "Document 12 is in cluster label 10\n",
      "Document 13 is in cluster label 7\n",
      "Document 14 is in cluster label 6\n",
      "Document 15 is in cluster label 10\n",
      "Document 16 is in cluster label 10\n",
      "Document 17 is in cluster label 10\n"
     ]
    }
   ],
   "source": [
    "for i, label in enumerate(cluster_labels):\n",
    "    print(f\"Document {i+1} is in cluster label {label+1}\")\n",
    "    if i>15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df[\"category\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WELLNESS',\n",
       " 'WELLNESS',\n",
       " 'WELLNESS',\n",
       " 'WELLNESS',\n",
       " 'WELLNESS',\n",
       " 'WELLNESS',\n",
       " 'WELLNESS',\n",
       " 'WELLNESS',\n",
       " 'WELLNESS',\n",
       " 'WELLNESS']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_score_knn = adjusted_rand_score(categories, cluster_labels)\n",
    "nmi_score_knn = normalized_mutual_info_score(categories, cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI Score: 0.0029680657766448597\n",
      "NMI Score: 0.01670519424450223\n"
     ]
    }
   ],
   "source": [
    "print(f\"ARI Score: {ari_score_knn}\")\n",
    "print(f\"NMI Score: {nmi_score_knn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35403221816380914"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating choerence score of \n",
    "# trained lda model\n",
    "coherence_model_lda = CoherenceModel(\n",
    "                                    model=lda_model,\n",
    "                                    texts=copy_df[\"clean_text\"],\n",
    "                                    dictionary=word_dict,\n",
    "                                    coherence=\"c_v\")\n",
    "coherence_score = coherence_model_lda.get_coherence()\n",
    "coherence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now creating lda soft cluster\n",
    "# for future reference constructing\n",
    "# the list comprehension and commenting it\n",
    "# lda_soft_clusters = max(lda_model[doc], key=lambda x: x[1])[0] for doc in copy_df[\"doc2bow\"]\n",
    "lda_categories = []\n",
    "for i, doc in enumerate(copy_df[\"doc2bow\"]):\n",
    "    # generating topic with word proba.\n",
    "    doc_topic = lda_model[doc]\n",
    "    # finding out the dominant topic by inspecting\n",
    "    # second element of the tuple which is probability\n",
    "    # then if maximum proba. found, it is selected\n",
    "    dominant_topic = max(doc_topic, key=lambda x: x[1])[0]\n",
    "    lda_categories.append(dominant_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again calculating ari and nmi scores\n",
    "ari_score_lda = adjusted_rand_score(categories, lda_categories)\n",
    "nmi_score_lda = adjusted_rand_score(categories, lda_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI Score: 0.027586858783359265\n",
      "NMI Score: 0.027586858783359265\n"
     ]
    }
   ],
   "source": [
    "print(f\"ARI Score: {ari_score_lda}\")\n",
    "print(f\"NMI Score: {nmi_score_lda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even thouh the coherence score is around: 0.3790 (approx), \n",
    "the LDA is not tuned in that way but it is still giving \n",
    "ARI and NMI score 0.0276 (approx) which is better \n",
    "than ARI score(0.0029) of KNN clustering (0.0167 approx) meaning, \n",
    "KNN is randomly assigning whereas LDA is finding very few but still some \n",
    "relations between the topics and the words to find the document to topics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
